{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "  \n",
    "train_files = ['data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5']\n",
    "images = np.array([],dtype=np.uint8).reshape((0,3072))\n",
    "labels = np.array([])\n",
    "for tf in train_files:\n",
    "    data_dict = unpickle('/content/cifar-10-batches-py/'+tf)\n",
    "    data = data_dict[b'data']\n",
    "    images = np.append(images,data,axis=0)\n",
    "    labels = np.append(labels,data_dict[b'labels'])\n",
    "#print(images.shape, labels.shape)\n",
    "\n",
    "testimages = np.array([],dtype=np.uint8).reshape((0,3072))\n",
    "testlabels = np.array([])\n",
    "\n",
    "data_dict = unpickle('/content/cifar-10-batches-py/test_batch')\n",
    "data = data_dict[b'data']\n",
    "testimages = np.append(testimages,data,axis=0)\n",
    "testlabels = np.append(testlabels,data_dict[b'labels'])\n",
    "#print(testimages.shape, testlabels.shape)\n",
    "\n",
    "images = images.reshape((-1,3,32,32)).astype(np.float)\n",
    "testimages = testimages.reshape((-1,3,32,32)).astype(np.float)\n",
    "\n",
    "lab_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',6:'frog',7:'horse',8:'ship',9:'truck'}\n",
    "frozen feature extractor and fine-tuning settings\n",
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "train=pd.read_csv(\"R/Data/Train/train.csv\")\n",
    "test=pd.read_csv(\"R/Data/test.csv\")\n",
    "train_path=\"R/Data/Train/Images/train/\"\n",
    "test_path=\"R/Data/Train/Images/test/\"\n",
    "\n",
    "from scipy.misc import imresize\n",
    "# preparing the train dataset\n",
    "\n",
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "\n",
    "    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))\n",
    "\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "\n",
    "    train_img.append(temp_img)\n",
    "\n",
    "#converting train images to array and applying mean subtraction processing\n",
    "\n",
    "train_img=np.array(train_img)\n",
    "train_img=preprocess_input(train_img)\n",
    "# applying the same procedure with the test dataset\n",
    "\n",
    "test_img=[]\n",
    "for i in range(len(test)):\n",
    "\n",
    "    temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))\n",
    "\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "\n",
    "    test_img.append(temp_img)\n",
    "\n",
    "test_img=np.array(test_img)\n",
    "test_img=preprocess_input(test_img)\n",
    "\n",
    "# loading VGG16 model weights\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "\n",
    "features_train=model.predict(train_img)\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "\n",
    "features_test=model.predict(test_img)\n",
    "\n",
    "# flattening the layers to conform to MLP input\n",
    "\n",
    "train_x=features_train.reshape(49000,25088)\n",
    "# converting target variable to array\n",
    "\n",
    "train_y=np.asarray(train['label'])\n",
    "# performing one-hot encoding for the target variable\n",
    "\n",
    "train_y=pd.get_dummies(train_y)\n",
    "train_y=np.array(train_y)\n",
    "# creating training and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,test_size=0.3, random_state=42)\n",
    "\n",
    " \n",
    "\n",
    "# creating a mlp model\n",
    "from keras.layers import Dense, Activation\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(1000, input_dim=25088, activation='relu',kernel_initializer='uniform'))\n",
    "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# fitting the model \n",
    "\n",
    "model.fit(X_train, Y_train, epochs=20, batch_size=128,validation_data=(X_valid,Y_valid))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "train=pd.read_csv(\"R/Data/Train/train.csv\")\n",
    "test=pd.read_csv(\"R/Data/test.csv\")\n",
    "train_path=\"R/Data/Train/Images/train/\"\n",
    "test_path=\"R/Data/Train/Images/test/\"\n",
    "\n",
    "from scipy.misc import imresize\n",
    "\n",
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "\n",
    "    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))\n",
    "\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "\n",
    "    train_img.append(temp_img)\n",
    "\n",
    "train_img=np.array(train_img)\n",
    "train_img=preprocess_input(train_img)\n",
    "\n",
    "test_img=[]\n",
    "for i in range(len(test)):\n",
    "\n",
    "temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))\n",
    "\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "\n",
    "    test_img.append(temp_img)\n",
    "\n",
    "test_img=np.array(test_img)\n",
    "test_img=preprocess_input(test_img)\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "def vgg16_model(img_rows, img_cols, channel=1, num_classes=None):\n",
    "\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    model.layers.pop()\n",
    "\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "\n",
    "          x=Dense(num_classes, activation='softmax')(model.output)\n",
    "\n",
    "    model=Model(model.input,x)\n",
    "\n",
    "#To set the first 8 layers to non-trainable (weights will not be updated)\n",
    "\n",
    "          for layer in model.layers[:8]:\n",
    "\n",
    "       layer.trainable = False\n",
    "\n",
    "# Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "train_y=np.asarray(train['label'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_y = le.fit_transform(train_y)\n",
    "\n",
    "train_y=to_categorical(train_y)\n",
    "\n",
    "train_y=np.array(train_y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)\n",
    "\n",
    "# Example to fine-tune on 3000 samples from Cifar10\n",
    "\n",
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 10 \n",
    "batch_size = 16\n",
    "nb_epoch = 10\n",
    "\n",
    "# Load our model\n",
    "model = vgg16_model(img_rows, img_cols, channel, num_classes)\n",
    "\n",
    "model.summary()\n",
    "# Start Fine-tuning\n",
    "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
    "\n",
    "# Make predictions\n",
    "predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Cross-entropy loss score\n",
    "score = log_loss(Y_valid, predictions_valid)\n",
    "dataset set:\n",
    "<1 x coarse label><1 x fine label><3072 x pixel>\n",
    "...\n",
    "<1 x coarse label><1 x fine label><3072 x pixel>\n",
    "Overall Accuracy for 100 categories of CIFAR100 test dataset\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
